{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-fr-lang1=en,lang2=fr\n",
      "Reusing dataset kde4 (C:\\Users\\asus\\.cache\\huggingface\\datasets\\kde4\\en-fr-lang1=en,lang2=fr\\0.0.0\\243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411521833df146e99b9d60f0b2433fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\asus\\.cache\\huggingface\\datasets\\kde4\\en-fr-lang1=en,lang2=fr\\0.0.0\\243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac\\cache-496be247a58b47c1.arrow and C:\\Users\\asus\\.cache\\huggingface\\datasets\\kde4\\en-fr-lang1=en,lang2=fr\\0.0.0\\243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac\\cache-2c0faebb61cdd12e.arrow\n"
     ]
    }
   ],
   "source": [
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '152754',\n",
       " 'translation': {'en': 'Default to expanded threads',\n",
       "  'fr': 'Par défaut, développer les fils de discussion'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081425a00f514af19da6521ebde8fa52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "29903961"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"].to_json(\"../data/train_datasets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13c186531c549b9bcdb257307b40c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3317985"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"validation\"].to_json(\"../data/eval_datasets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl_files(filepath):\n",
    "    result = []\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for x in jsonlines.Reader(f):\n",
    "            result.append(x)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_info(text_list, tokenizer):\n",
    "    \n",
    "    token2oldId = {}\n",
    "    for text in tqdm(text_list, leave=False):\n",
    "        tokenized_text = tokenizer(text)\n",
    "\n",
    "        tokens = tokenized_text.tokens()\n",
    "        id_list = tokenized_text.input_ids\n",
    "\n",
    "        for token, idx in zip(tokens, id_list):\n",
    "            token2oldId[token] = idx\n",
    "    \n",
    "    pad_token = tokenizer.pad_token\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    if pad_token not in token2oldId:\n",
    "        token2oldId[pad_token] = pad_token_id\n",
    "\n",
    "    num_tokens = len(token2oldId)\n",
    "    tokens = list(token2oldId.keys())\n",
    "    tokens.remove(pad_token)\n",
    "    tokens = [pad_token] + tokens\n",
    "\n",
    "    token2newId = {}\n",
    "    old2new = {}\n",
    "    new2old = {}\n",
    "\n",
    "    for new_id, token in enumerate(tokens):\n",
    "        token2newId[token] = new_id\n",
    "        old_id = token2oldId[token]\n",
    "        old2new[old_id] = new_id\n",
    "        new2old[new_id] = old_id\n",
    "    \n",
    "    token_info = {\n",
    "        \"token2oldId\": token2oldId,\n",
    "        \"token2newId\": token2newId,\n",
    "        \"old2new\": old2new,\n",
    "        \"new2old\": new2old,\n",
    "    }\n",
    "\n",
    "    return token_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_info_main():\n",
    "    eval_data = read_jsonl_files(\"../data/eval_datasets.json\")\n",
    "    train_data = read_jsonl_files(\"../data/train_datasets.json\")\n",
    "\n",
    "    en_text_list = [x[\"translation\"][\"en\"] for x in eval_data] + [x[\"translation\"][\"en\"] for x in train_data]\n",
    "    fr_text_list = [x[\"translation\"][\"fr\"] for x in eval_data] + [x[\"translation\"][\"fr\"] for x in train_data] \n",
    "\n",
    "    en_token_info = get_token_info(en_text_list, tokenizer)\n",
    "    fr_token_info = get_token_info(fr_text_list, tokenizer)\n",
    "\n",
    "    with open(\"../data/en_token_info.json\", \"w\") as f:\n",
    "        json.dump(en_token_info, f)\n",
    "    \n",
    "    with open(\"../data/fr_token_info.json\", \"w\") as f:\n",
    "        json.dump(fr_token_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a763cc528cb4476da29653a9938518f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430b19782cec4abdad6141a29afb6292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_token_info_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442db79d33b543b39359ed572640720b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "eval_data = read_jsonl_files(\"../data/eval_datasets.json\")\n",
    "train_data = read_jsonl_files(\"../data/train_datasets.json\")\n",
    "\n",
    "en_text_list = [x[\"translation\"][\"en\"] for x in eval_data] + [x[\"translation\"][\"en\"] for x in train_data]\n",
    "fr_text_list = [x[\"translation\"][\"fr\"] for x in eval_data] + [x[\"translation\"][\"fr\"] for x in train_data] \n",
    "\n",
    "en_token_info = get_token_info(en_text_list, tokenizer)\n",
    "# fr_token_info = get_token_info(fr_text_list, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d364c3516e04dbde6d0194e2c30cb4fb922514bc2c4c3e020a3ce93bbd388d78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
